{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 段落斷詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "Package not found at '0.醫材研討會議程.pptx'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-8d06585e7075>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpptx\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPresentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPresentation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# text_runs will be populated with a list of strings,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# one for each text run in presentation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtext_runs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.5\\lib\\site-packages\\pptx\\api.py\u001b[0m in \u001b[0;36mPresentation\u001b[1;34m(pptx)\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mpptx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_default_pptx_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0mpresentation_part\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpptx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain_document_part\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_is_pptx_package\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpresentation_part\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.5\\lib\\site-packages\\pptx\\opc\\package.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \"\"\"\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mpkg_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m         \u001b[0mpackage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         \u001b[0mUnmarshaller\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munmarshal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPartFactory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.5\\lib\\site-packages\\pptx\\opc\\pkgreader.py\u001b[0m in \u001b[0;36mfrom_file\u001b[1;34m(pkg_file)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m|\u001b[0m\u001b[0mPackageReader\u001b[0m\u001b[1;33m|\u001b[0m \u001b[0minstance\u001b[0m \u001b[0mloaded\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mcontents\u001b[0m \u001b[0mof\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \"\"\"\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mphys_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhysPkgReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpkg_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mcontent_types\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_ContentTypeMap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_xml\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent_types_xml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mpkg_srels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPackageReader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_srels_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphys_reader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPACKAGE_URI\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\python 3.5\\lib\\site-packages\\pptx\\opc\\phys_pkg.py\u001b[0m in \u001b[0;36m__new__\u001b[1;34m(cls, pkg_file)\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m                 raise PackageNotFoundError(\n\u001b[1;32m---> 32\u001b[1;33m                     \u001b[1;34m\"Package not found at '%s'\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mpkg_file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m                 )\n\u001b[0;32m     34\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# assume it's a stream and pass it to Zip reader to sort out\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPackageNotFoundError\u001b[0m: Package not found at '0.醫材研討會議程.pptx'"
     ]
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "prs = Presentation(fileName)\n",
    "# text_runs will be populated with a list of strings,\n",
    "# one for each text run in presentation\n",
    "text_runs = []\n",
    "for slide in prs.slides:\n",
    "    #print('slide = %s' %slide)\n",
    "    for shape in slide.shapes:\n",
    "        #print('shape.has_text_frame = %s' %shape.has_text_frame)\n",
    "        if not shape.has_text_frame:\n",
    "            continue\n",
    "        for paragraph in shape.text_frame.paragraphs:\n",
    "            if not paragraph.text:\n",
    "                continue\n",
    "            #print('paragraph.text = %s' %paragraph.text)\n",
    "            for run in paragraph.runs:\n",
    "                # 判斷是否為空字串 if not run :不對寫法!\n",
    "                if (run.text !=' '):\n",
    "                    run.text = run.text.rstrip()\n",
    "                    run.text = run.text.lstrip()\n",
    "                    print('顯示run=%s' %run.text)\n",
    "                    #print('run.text = %s' %run.text)\n",
    "                    text_runs.append(run.text)\n",
    "len(text_runs)\n",
    "# text_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 寫入並輸出Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import xlsxwriter, time\n",
    "\n",
    "outName = 'outputExcel_'\n",
    "s = time.strftime(\"%m%d-%H_%M_%S\", time.localtime())  #檔名\n",
    "workbook = xlsxwriter.Workbook(outName+fileName+'_'+s+'.xlsx')  #建立檔案\n",
    "worksheet = workbook.add_worksheet()\n",
    "# Widen the first column to make the text clearer.\n",
    "worksheet.set_column('A:M', 15)\n",
    "\n",
    "# Start from the first cell. Rows and columns are zero indexed.\n",
    "row = 0\n",
    "col = 0\n",
    "''' 去除重複部分 '''\n",
    "text_runs_dict = set(text_runs)\n",
    "print(text_runs_dict)\n",
    "print(len(text_runs_dict))\n",
    "\n",
    "for item in text_runs_dict : #text_runs:\n",
    "    #print(item)\n",
    "    if not item:\n",
    "        print('顯示item = %s' %item)\n",
    "        continue\n",
    "    else:\n",
    "        worksheet.write(row, col, item)\n",
    "        row+=1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "test_string = 'aabb abb aab '\n",
    "pattern = 'a+b'\n",
    "ans=re.findall(pattern,test_string)'''\n",
    "lineAll = text_runs_str\n",
    "# pattern = re.compile(r's+')\n",
    "# text_runs_strTemp = re.sub(pattern, '', text_runs_strTemp)\n",
    "lines = [line.strip() for line in text_runs]\n",
    "for line in lines:\n",
    "    print(line)\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['醫材技術 研討會',\n",
       " '\\u200b\\u200cClassify:AUO-General\\u200c\\u200b',\n",
       " '醫材技術 研討會',\n",
       " '時間: 2018年12月12日\\n地點: Qisda 桃園總部 A70\\n主辦單位: 明基友達集團 GRD 醫材委員會',\n",
       " '\\u200b\\u200cClassify:AUO-General\\u200c\\u200b',\n",
       " '\\u200b\\u200cClassify:AUO-General\\u200c\\u200b',\n",
       " '\\u200b\\u200cClassify:AUO-General\\u200c\\u200b']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pptx import Presentation\n",
    "# 03_Ivan_X-ray sensor_20181123.pptx / VRTAA3 - Monthly Report 2018-1123.pptx\n",
    "path = 'D:\\\\Code\\\\Project\\\\CrawlerProj_Anson_Python\\\\ettoday_SVN\\\\RD Document\\\\' \n",
    "fileName = '0.醫材研討會議程.pptx'\n",
    "\n",
    "''' 讀pptx檔轉txt'''    \n",
    "def GetallText(fileName):\n",
    "    prs = Presentation(fileName)\n",
    "    AllTextList = []\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if not shape.has_text_frame: #判斷shape是否為空 \n",
    "                continue\n",
    "            txt = shape.text # 這裡一定要用變數存shape.text，不然會錯!\n",
    "            try:\n",
    "                #print(shape.text)\n",
    "                if txt !=' 'and txt !='': \n",
    "                    AllTextList.append(txt)\n",
    "            except AttributeError:\n",
    "                print(\"error\")\n",
    "                continue\n",
    "    return AllTextList\n",
    "\n",
    "AllText_list = GetallText(path+fileName)        \n",
    "AllText_str = ''.join(AllText_list)\n",
    "AllText_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用jieba 斷詞 並加入自訂義字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "顯示斷完結果2：\n",
      "醫材 技術   研討會 ​ ‌ Classify : AUO - General ‌ ​ 醫材 技術   研討會 時間 :   2018 年 12 月 12 日 \n",
      " 地點 :   Qisda   桃園 總部   A70 \n",
      " 主辦 單位 :   明基 友達 集團   GRD   醫材 委員會 ​ ‌ Classify : AUO - General ‌ ​ ​ ‌ Classify : AUO - General ‌ ​ ​ ‌ Classify : AUO - General ‌ ​\n",
      "顯示kw = 2018, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 研討會, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 集團, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 明基, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = Qisda, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 桃園, IDF = 9.37888907493\n",
      "\n",
      "顯示kw = 時間, IDF = 4.07953963246\n",
      "\n",
      "顯示kw = 醫材, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 友達, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 12, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = GRD, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 主辦, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 技術, IDF = 4.71945717857\n",
      "\n",
      "顯示kw = 總部, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 委員會, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = A70, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 地點, IDF = 11.2616203224\n",
      "\n",
      "顯示kw = 單位, IDF = 11.2616203224\n",
      "\n",
      "顯示freq個數 = 18\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([('醫材', 1.4689069985739132),\n",
       "  ('研討會', 0.9792713323826088),\n",
       "  ('12', 0.9792713323826088),\n",
       "  ('2018', 0.4896356661913044),\n",
       "  ('集團', 0.4896356661913044),\n",
       "  ('明基', 0.4896356661913044),\n",
       "  ('Qisda', 0.4896356661913044),\n",
       "  ('友達', 0.4896356661913044),\n",
       "  ('GRD', 0.4896356661913044),\n",
       "  ('主辦', 0.4896356661913044),\n",
       "  ('總部', 0.4896356661913044),\n",
       "  ('委員會', 0.4896356661913044),\n",
       "  ('A70', 0.4896356661913044),\n",
       "  ('地點', 0.4896356661913044),\n",
       "  ('單位', 0.4896356661913044),\n",
       "  ('技術', 0.41038758074521736),\n",
       "  ('桃園', 0.4077777858665217),\n",
       "  ('時間', 0.1773712883678261)],\n",
       " {'12': 0.9792713323826088,\n",
       "  '2018': 0.4896356661913044,\n",
       "  'A70': 0.4896356661913044,\n",
       "  'GRD': 0.4896356661913044,\n",
       "  'Qisda': 0.4896356661913044,\n",
       "  '主辦': 0.4896356661913044,\n",
       "  '友達': 0.4896356661913044,\n",
       "  '單位': 0.4896356661913044,\n",
       "  '地點': 0.4896356661913044,\n",
       "  '委員會': 0.4896356661913044,\n",
       "  '技術': 0.41038758074521736,\n",
       "  '明基': 0.4896356661913044,\n",
       "  '時間': 0.1773712883678261,\n",
       "  '桃園': 0.4077777858665217,\n",
       "  '研討會': 0.9792713323826088,\n",
       "  '總部': 0.4896356661913044,\n",
       "  '醫材': 1.4689069985739132,\n",
       "  '集團': 0.4896356661913044},\n",
       " 23.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jieba\n",
    "# DictFile_name = 'jieba\\\\default_dict.txt' # 注意: 這裡要\\\\，會有unicode編碼問題\n",
    "# jieba.load_userdict(DictFile_name) # DictFile_name 为文件类对象或自定义词典的路径\n",
    "\n",
    "# words = jieba.cut(Alltext_str)\n",
    "# print(\"顯示斷完結果1：\\n\"+' '.join(words))\n",
    "\n",
    "sentence = AllText_str\n",
    "words2 = jieba.lcut(sentence)\n",
    "print(\"顯示斷完結果2：\\n\"+' '.join(words2))\n",
    "\n",
    "import jieba.analyse\n",
    "sentence = AllText_str\n",
    "tfidf_list, freq, total = jieba.analyse.extract_tags(sentence, topK=50, withWeight = True, \\\n",
    "                                        allowPOS=())\n",
    "(tfidf_list, freq, total)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-4692afad9404>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# str(keywordsCnt_list[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-4692afad9404>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtfidf_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfreq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# str(keywordsCnt_list[0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtmp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "tfidf_list[0]\n",
    "freq = [ list(freq[i]) for i in range(len(freq)) ] \n",
    "# str(keywordsCnt_list[0])\n",
    "print(freq)\n",
    "tmp = freq[0]\n",
    "tmp[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq\n",
    "freq = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = keywordsCnt_list[1][0]\n",
    "tt\n",
    "keywordsCnt_list_w = [w for w in keywordsCnt_list[i][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time, xlsxwriter\n",
    "time_str = time.strftime(\"%m%d-%H_%M_%S\", time.localtime())  #檔名\n",
    "workbook = xlsxwriter.Workbook('TFIDF_result\\\\' + fileName +'_'+time_str+'.xlsx')#建立檔案\n",
    "worksheet = workbook.add_worksheet()\n",
    "worksheet.set_column('A:M', 15)\n",
    "# Start from the first cell. Rows and columns are zero indexed.\n",
    "row = 0\n",
    "col = 0\n",
    "for row, line in enumerate(tfidf_list):\n",
    "    print('顯示row=%s, line = %s' %(row, line))\n",
    "    for col, cell in enumerate(line):\n",
    "        print('col = %s, cell = %s' %(col, cell))\n",
    "        if not cell:\n",
    "            continue\n",
    "        elif (type(cell)==float): # float=idf value\n",
    "            worksheet.write(row, col, cell)\n",
    "            for w in keywordsCnt_list:\n",
    "                if w\n",
    "            print('tmp = ' , tmp)\n",
    "            worksheet.write(row, col+1, tmp[1])\n",
    "            #row+=1\n",
    "        else: # 文字\n",
    "            worksheet.write(row, col, cell)\n",
    "            #row+=1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_list\n",
    "tfidf_dict = dict(tfidf_list)\n",
    "# for item in tfidf_dict.keys():\n",
    "#     print(item)\n",
    "tfidf_keys = tfidf_dict.keys()\n",
    "type(tfidf_keys)\n",
    "tfidf_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_list2 = []\n",
    "for i in range(len(tfidf_list)):\n",
    "    tfidf_list2.append(list(tfidf_list[i]))\n",
    "\n",
    "tfidf_list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "KW = \n",
    "tfidf_list2[:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[ val for val in sorted(tfidf_dict.values(), reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf_keys = list(tfidf_keys)\n",
    "\n",
    "IDF_list = [ [tfidf_keys[ix], IDF_all_list[tfidf_keys[ix]]] for ix in range(len(tfidf_keys))]\n",
    "IDF_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time, xlsxwriter\n",
    "def KeyWord2Excel(keywords_list):\n",
    "    outName = 'outputExcel_'\n",
    "    time_str = time.strftime(\"%m%d-%H_%M_%S\", time.localtime())  #檔名\n",
    "    workbook = xlsxwriter.Workbook(outName+fileName+'_'+time_str+'.xlsx')  #建立檔案\n",
    "    worksheet = workbook.add_worksheet()\n",
    "    # Widen the first column to make the text clearer.\n",
    "    worksheet.set_column('A:M', 15)\n",
    "    # Start from the first cell. Rows and columns are zero indexed.\n",
    "    row = 0\n",
    "    col = 0\n",
    "    print(keywords_list)\n",
    "    print('顯示個數 = %s' %(len(keywords_list)))\n",
    "    \n",
    "    worksheet.write(row, col, 'Top關鍵字')\n",
    "    worksheet.write(row, col+1, 'TFIDF值')\n",
    "    row+=1\n",
    "    for item in keywords_list :\n",
    "        #print('顯示item = %s' %(item,))\n",
    "        if not item:\n",
    "            continue\n",
    "        else:\n",
    "            worksheet.write(row, col, item[0])\n",
    "            worksheet.write(row, col+1, item[1])\n",
    "            row+=1\n",
    "    workbook.close()\n",
    "\n",
    "#KeyWord2Excel(KeyWords_list)   \n",
    "\n",
    "KeyWord2Excel(keywordsCnt_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDF_all_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KeyWord2Excel(tfidf_list)\n",
    "KeyWord2Excel(IDF_all_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# words22 = jieba.cut(Alltext_str)\n",
    "# print(\"顯示斷完結果：\\n\"+' '.join(words22))\n",
    "fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "import xlsxwriter, time\n",
    "import jieba\n",
    "\n",
    "time_str = time.strftime(\"%m%d-%H_%M_%S\", time.localtime())  #檔名\n",
    "workbook = xlsxwriter.Workbook(fileName+'_'+time_str+'.xlsx')#建立檔案\n",
    "worksheet = workbook.add_worksheet()\n",
    "worksheet.set_column('A:M', 15)\n",
    "# Start from the first cell. Rows and columns are zero indexed.\n",
    "row = 0\n",
    "col = 0\n",
    "for row, line in enumerate(tfidf_list):\n",
    "    print('顯示item = %s' %(line, ))\n",
    "    for col, cell in enumerate(line):\n",
    "        print('顯示row, col, cell = (%s %s %s)' %(row, col, cell))\n",
    "        if not cell:\n",
    "            continue\n",
    "        elif (type(cell)==float):\n",
    "            worksheet.write(row, cell)\n",
    "            #row+=1\n",
    "        else:\n",
    "            #worksheet.write(row, col, list(item[row]))\n",
    "            worksheet.write(row, col, cell)\n",
    "            #row+=1\n",
    "workbook.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyWord2Excel(words)    \n",
    "KeyWord2Excel(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 從PDF 擷取文字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "pdfFileObj = open('108碩甄第8次備取遞補名單.pdf', 'rb')\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "pdfReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdfReader.numPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paegeObj = pdfReader.getPage(0) # 0 代表第一頁!\n",
    "paegeObj.extractText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''下載NLTK套件'''\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import sinica_treebank\n",
    "sinica_text = nltk.Text(sinica_treebank.words())\n",
    "sinica_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sinica_text = nltk.Text(sinica_treebank.words())\n",
    "sinica_text.concordance('我')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "input_file = \"article/zhwiki-latest-pages-articles.xml.bz2\"\n",
    "f = open('article/zhwiki.txt',encoding='utf8',mode='w')\n",
    "wiki =  gensim.corpora.WikiCorpus(input_file, lemmatize=False, dictionary={})\n",
    "for text in wiki.get_texts():\n",
    "    str_line = ' '.join(text)\n",
    "    f.write(str_line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### https://www.bogotobogo.com/python/NLTK/tf_idf_with_scikit-learn_NLTK.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "path = './tf-idf'\n",
    "token_dict = {}\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stems = []\n",
    "    for item in tokens:\n",
    "        stems.append(PorterStemmer().stem(item))\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'D:\\\\nltk_data' #os.getcwd()\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        print (\"fname=\", fname)\n",
    "        if fname.split('.')[-1]=='txt':\n",
    "            with open(fname, mode='r', encoding='utf-8') as doc:\n",
    "                text = doc.read()\n",
    "                token_dict[f] = text.lower().translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用open(fname, mode='rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "path = 'tf-idf' #os.getcwd()\n",
    "for dirpath, dirs, files in os.walk(path):\n",
    "    for f in files:\n",
    "        fname = os.path.join(dirpath, f)\n",
    "        print (\"fname=\", fname)\n",
    "        if fname.split('.')[-1]=='txt':\n",
    "            with open(fname, mode='rb') as doc:\n",
    "                text = doc.read()\n",
    "                try:\n",
    "                    text_str = text.decode('utf-8')\n",
    "                    token_dict[f] = text_str.lower().translate(string.punctuation)\n",
    "                except UnicodeDecodeError:\n",
    "                    token_dict[f] = text_str.lower().translate(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=tokenize, stop_words='english')\n",
    "tfs = tfidf.fit_transform(token_dict.values())\n",
    "\n",
    "str = 'all great and precious things are lonely.'\n",
    "response = tfidf.transform([str])\n",
    "print(response)\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "for col in response.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', response[0, col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(text.lower().translate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname_list = fname.split('.')\n",
    "# fname_list[-1]\n",
    "fname.split('.')[-1]\n",
    "if fname.split('.')[-1]!='zip':\n",
    "    print('can read')\n",
    "else:\n",
    "    print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "from six import text_type\n",
    "\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.probability import ConditionalFreqDist as CFD\n",
    "from nltk.util import tokenwrap, LazyConcatenation\n",
    "from nltk.metrics import f_measure, BigramAssocMeasures\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.compat import python_2_unicode_compatible\n",
    "\n",
    "ConcordanceLine = namedtuple(\n",
    "    'ConcordanceLine',\n",
    "    ['left', 'query', 'right', 'offset', 'left_print', 'right_print', 'line'],\n",
    ")\n",
    "\n",
    "@python_2_unicode_compatible\n",
    "class Text(object):\n",
    "    \"\"\"\n",
    "    A wrapper around a sequence of simple (string) tokens, which is\n",
    "    intended to support initial exploration of texts (via the\n",
    "    interactive console).  Its methods perform a variety of analyses\n",
    "    on the text's contexts (e.g., counting, concordancing, collocation\n",
    "    discovery), and display the results.  If you wish to write a\n",
    "    program which makes use of these analyses, then you should bypass\n",
    "    the ``Text`` class, and use the appropriate analysis function or\n",
    "    class directly instead.\n",
    "\n",
    "    A ``Text`` is typically initialized from a given document or\n",
    "    corpus.  E.g.:\n",
    "\n",
    "    >>> import nltk.corpus\n",
    "    >>> from nltk.text import Text\n",
    "    >>> moby = Text(nltk.corpus.gutenberg.words('melville-moby_dick.txt'))\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # This defeats lazy loading, but makes things faster.  This\n",
    "    # *shouldn't* be necessary because the corpus view *should* be\n",
    "    # doing intelligent caching, but without this it's running slow.\n",
    "    # Look into whether the caching is working correctly.\n",
    "    _COPY_TOKENS = True\n",
    "\n",
    "    def __init__(self, tokens, name=None):\n",
    "        \"\"\"\n",
    "        Create a Text object.\n",
    "\n",
    "        :param tokens: The source text.\n",
    "        :type tokens: sequence of str\n",
    "        \"\"\"\n",
    "        if self._COPY_TOKENS:\n",
    "            tokens = list(tokens)\n",
    "        self.tokens = tokens\n",
    "\n",
    "        if name:\n",
    "            self.name = name\n",
    "        elif ']' in tokens[:20]:\n",
    "            end = tokens[:20].index(']')\n",
    "            self.name = \" \".join(text_type(tok) for tok in tokens[1:end])\n",
    "        else:\n",
    "            self.name = \" \".join(text_type(tok) for tok in tokens[:8]) + \"...\"\n",
    "\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "    # Support item & slice access\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.tokens[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "    # Interactive console methods\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "\n",
    "    def concordance(self, word, width=79, lines=25):\n",
    "        \"\"\"\n",
    "        Prints a concordance for ``word`` with the specified context window.\n",
    "        Word matching is not case-sensitive.\n",
    "\n",
    "        :param word: The target word\n",
    "        :type word: str\n",
    "        :param width: The width of each line, in characters (default=80)\n",
    "        :type width: int\n",
    "        :param lines: The number of lines to display (default=25)\n",
    "        :type lines: int\n",
    "\n",
    "        :seealso: ``ConcordanceIndex``\n",
    "        \"\"\"\n",
    "        if '_concordance_index' not in self.__dict__:\n",
    "            self._concordance_index = ConcordanceIndex(\n",
    "                self.tokens, key=lambda s: s.lower()\n",
    "            )\n",
    "\n",
    "        return self._concordance_index.print_concordance(word, width, lines)\n",
    "\n",
    "\n",
    "    def concordance_list(self, word, width=79, lines=25):\n",
    "        \"\"\"\n",
    "        Generate a concordance for ``word`` with the specified context window.\n",
    "        Word matching is not case-sensitive.\n",
    "\n",
    "        :param word: The target word\n",
    "        :type word: str\n",
    "        :param width: The width of each line, in characters (default=80)\n",
    "        :type width: int\n",
    "        :param lines: The number of lines to display (default=25)\n",
    "        :type lines: int\n",
    "\n",
    "        :seealso: ``ConcordanceIndex``\n",
    "        \"\"\"\n",
    "        if '_concordance_index' not in self.__dict__:\n",
    "            self._concordance_index = ConcordanceIndex(\n",
    "                self.tokens, key=lambda s: s.lower()\n",
    "            )\n",
    "        return self._concordance_index.find_concordance(word, width)[:lines]\n",
    "\n",
    "\n",
    "    def collocations(self, num=20, window_size=2):\n",
    "        \"\"\"\n",
    "        Print collocations derived from the text, ignoring stopwords.\n",
    "\n",
    "        :seealso: find_collocations\n",
    "        :param num: The maximum number of collocations to print.\n",
    "        :type num: int\n",
    "        :param window_size: The number of tokens spanned by a collocation (default=2)\n",
    "        :type window_size: int\n",
    "        \"\"\"\n",
    "        if not (\n",
    "            '_collocations' in self.__dict__\n",
    "            and self._num == num\n",
    "            and self._window_size == window_size\n",
    "        ):\n",
    "            self._num = num\n",
    "            self._window_size = window_size\n",
    "\n",
    "            # print(\"Building collocations list\")\n",
    "            from nltk.corpus import stopwords\n",
    "\n",
    "            ignored_words = stopwords.words('english')\n",
    "            finder = BigramCollocationFinder.from_words(self.tokens, window_size)\n",
    "            finder.apply_freq_filter(2)\n",
    "            finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)\n",
    "            bigram_measures = BigramAssocMeasures()\n",
    "            self._collocations = finder.nbest(bigram_measures.likelihood_ratio, num)\n",
    "        colloc_strings = [w1 + ' ' + w2 for w1, w2 in self._collocations]\n",
    "        print(tokenwrap(colloc_strings, separator=\"; \"))\n",
    "\n",
    "\n",
    "    def count(self, word):\n",
    "        \"\"\"\n",
    "        Count the number of times this word appears in the text.\n",
    "        \"\"\"\n",
    "        return self.tokens.count(word)\n",
    "\n",
    "\n",
    "    def index(self, word):\n",
    "        \"\"\"\n",
    "        Find the index of the first occurrence of the word in the text.\n",
    "        \"\"\"\n",
    "        return self.tokens.index(word)\n",
    "\n",
    "\n",
    "    def readability(self, method):\n",
    "        # code from nltk_contrib.readability\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "    def similar(self, word, num=20):\n",
    "        \"\"\"\n",
    "        Distributional similarity: find other words which appear in the\n",
    "        same contexts as the specified word; list most similar words first.\n",
    "\n",
    "        :param word: The word used to seed the similarity search\n",
    "        :type word: str\n",
    "        :param num: The number of words to generate (default=20)\n",
    "        :type num: int\n",
    "        :seealso: ContextIndex.similar_words()\n",
    "        \"\"\"\n",
    "        if '_word_context_index' not in self.__dict__:\n",
    "            # print('Building word-context index...')\n",
    "            self._word_context_index = ContextIndex(\n",
    "                self.tokens, filter=lambda x: x.isalpha(), key=lambda s: s.lower()\n",
    "            )\n",
    "\n",
    "        # words = self._word_context_index.similar_words(word, num)\n",
    "\n",
    "        word = word.lower()\n",
    "        wci = self._word_context_index._word_to_contexts\n",
    "        if word in wci.conditions():\n",
    "            contexts = set(wci[word])\n",
    "            fd = Counter(\n",
    "                w\n",
    "                for w in wci.conditions()\n",
    "                for c in wci[w]\n",
    "                if c in contexts and not w == word\n",
    "            )\n",
    "            words = [w for w, _ in fd.most_common(num)]\n",
    "            print(tokenwrap(words))\n",
    "        else:\n",
    "            print(\"No matches\")\n",
    "\n",
    "\n",
    "    def common_contexts(self, words, num=20):\n",
    "        \"\"\"\n",
    "        Find contexts where the specified words appear; list\n",
    "        most frequent common contexts first.\n",
    "\n",
    "        :param word: The word used to seed the similarity search\n",
    "        :type word: str\n",
    "        :param num: The number of words to generate (default=20)\n",
    "        :type num: int\n",
    "        :seealso: ContextIndex.common_contexts()\n",
    "        \"\"\"\n",
    "        if '_word_context_index' not in self.__dict__:\n",
    "            # print('Building word-context index...')\n",
    "            self._word_context_index = ContextIndex(\n",
    "                self.tokens, key=lambda s: s.lower()\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            fd = self._word_context_index.common_contexts(words, True)\n",
    "            if not fd:\n",
    "                print(\"No common contexts were found\")\n",
    "            else:\n",
    "                ranked_contexts = [w for w, _ in fd.most_common(num)]\n",
    "                print(tokenwrap(w1 + \"_\" + w2 for w1, w2 in ranked_contexts))\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "    def dispersion_plot(self, words):\n",
    "        \"\"\"\n",
    "        Produce a plot showing the distribution of the words through the text.\n",
    "        Requires pylab to be installed.\n",
    "\n",
    "        :param words: The words to be plotted\n",
    "        :type words: list(str)\n",
    "        :seealso: nltk.draw.dispersion_plot()\n",
    "        \"\"\"\n",
    "        from nltk.draw import dispersion_plot\n",
    "\n",
    "        dispersion_plot(self, words)\n",
    "\n",
    "\n",
    "    def generate(self, words):\n",
    "        \"\"\"\n",
    "        Issues a reminder to users following the book online\n",
    "        \"\"\"\n",
    "        import warnings\n",
    "\n",
    "        warnings.warn(\n",
    "            'The generate() method is no longer available.', DeprecationWarning\n",
    "        )\n",
    "\n",
    "\n",
    "    def plot(self, *args):\n",
    "        \"\"\"\n",
    "        See documentation for FreqDist.plot()\n",
    "        :seealso: nltk.prob.FreqDist.plot()\n",
    "        \"\"\"\n",
    "        self.vocab().plot(*args)\n",
    "\n",
    "\n",
    "    def vocab(self):\n",
    "        \"\"\"\n",
    "        :seealso: nltk.prob.FreqDist\n",
    "        \"\"\"\n",
    "        if \"_vocab\" not in self.__dict__:\n",
    "            # print(\"Building vocabulary index...\")\n",
    "            self._vocab = FreqDist(self)\n",
    "        return self._vocab\n",
    "\n",
    "\n",
    "    def findall(self, regexp):\n",
    "        \"\"\"\n",
    "        Find instances of the regular expression in the text.\n",
    "        The text is a list of tokens, and a regexp pattern to match\n",
    "        a single token must be surrounded by angle brackets.  E.g.\n",
    "\n",
    "        >>> print('hack'); from nltk.book import text1, text5, text9\n",
    "        hack...\n",
    "        >>> text5.findall(\"<.*><.*><bro>\")\n",
    "        you rule bro; telling you bro; u twizted bro\n",
    "        >>> text1.findall(\"<a>(<.*>)<man>\")\n",
    "        monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
    "        mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
    "        pale; furious; better; certain; complete; dismasted; younger; brave;\n",
    "        brave; brave; brave\n",
    "        >>> text9.findall(\"<th.*>{3,}\")\n",
    "        thread through those; the thought that; that the thing; the thing\n",
    "        that; that that thing; through these than through; them that the;\n",
    "        through the thick; them that they; thought that the\n",
    "\n",
    "        :param regexp: A regular expression\n",
    "        :type regexp: str\n",
    "        \"\"\"\n",
    "\n",
    "        if \"_token_searcher\" not in self.__dict__:\n",
    "            self._token_searcher = TokenSearcher(self)\n",
    "\n",
    "        hits = self._token_searcher.findall(regexp)\n",
    "        hits = [' '.join(h) for h in hits]\n",
    "        print(tokenwrap(hits, \"; \"))\n",
    "\n",
    "\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "    # Helper Methods\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "\n",
    "    _CONTEXT_RE = re.compile('\\w+|[\\.\\!\\?]')\n",
    "\n",
    "    def _context(self, tokens, i):\n",
    "        \"\"\"\n",
    "        One left & one right token, both case-normalized.  Skip over\n",
    "        non-sentence-final punctuation.  Used by the ``ContextIndex``\n",
    "        that is created for ``similar()`` and ``common_contexts()``.\n",
    "        \"\"\"\n",
    "        # Left context\n",
    "        j = i - 1\n",
    "        while j >= 0 and not self._CONTEXT_RE.match(tokens[j]):\n",
    "            j -= 1\n",
    "        left = tokens[j] if j != 0 else '*START*'\n",
    "\n",
    "        # Right context\n",
    "        j = i + 1\n",
    "        while j < len(tokens) and not self._CONTEXT_RE.match(tokens[j]):\n",
    "            j += 1\n",
    "        right = tokens[j] if j != len(tokens) else '*END*'\n",
    "\n",
    "        return (left, right)\n",
    "\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "    # String Display\n",
    "    # ////////////////////////////////////////////////////////////\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<Text: %s>' % self.name\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<Text: %s>' % self.name\n",
    "\n",
    "class ContextIndex(object):\n",
    "    \"\"\"\n",
    "    A bidirectional index between words and their 'contexts' in a text.\n",
    "    The context of a word is usually defined to be the words that occur\n",
    "    in a fixed window around the word; but other definitions may also\n",
    "    be used by providing a custom context function.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def _default_context(tokens, i):\n",
    "        \"\"\"One left token and one right token, normalized to lowercase\"\"\"\n",
    "        left = tokens[i - 1].lower() if i != 0 else '*START*'\n",
    "        right = tokens[i + 1].lower() if i != len(tokens) - 1 else '*END*'\n",
    "        return (left, right)\n",
    "\n",
    "    def __init__(self, tokens, context_func=None, filter=None, key=lambda x: x):\n",
    "        self._key = key\n",
    "        self._tokens = tokens\n",
    "        if context_func:\n",
    "            self._context_func = context_func\n",
    "        else:\n",
    "            self._context_func = self._default_context\n",
    "        if filter:\n",
    "            tokens = [t for t in tokens if filter(t)]\n",
    "        self._word_to_contexts = CFD(\n",
    "            (self._key(w), self._context_func(tokens, i)) for i, w in enumerate(tokens)\n",
    "        )\n",
    "        self._context_to_words = CFD(\n",
    "            (self._context_func(tokens, i), self._key(w)) for i, w in enumerate(tokens)\n",
    "        )\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"\n",
    "        :rtype: list(str)\n",
    "        :return: The document that this context index was\n",
    "            created from.\n",
    "        \"\"\"\n",
    "        return self._tokens\n",
    "\n",
    "\n",
    "    def word_similarity_dict(self, word):\n",
    "        \"\"\"\n",
    "        Return a dictionary mapping from words to 'similarity scores,'\n",
    "        indicating how often these two words occur in the same\n",
    "        context.\n",
    "        \"\"\"\n",
    "        word = self._key(word)\n",
    "        word_contexts = set(self._word_to_contexts[word])\n",
    "\n",
    "        scores = {}\n",
    "        for w, w_contexts in self._word_to_contexts.items():\n",
    "            scores[w] = f_measure(word_contexts, set(w_contexts))\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "    def similar_words(self, word, n=20):\n",
    "        scores = defaultdict(int)\n",
    "        for c in self._word_to_contexts[self._key(word)]:\n",
    "            for w in self._context_to_words[c]:\n",
    "                if w != word:\n",
    "                    scores[w] += (\n",
    "                        self._context_to_words[c][word] * self._context_to_words[c][w]\n",
    "                    )\n",
    "        return sorted(scores, key=scores.get, reverse=True)[:n]\n",
    "\n",
    "\n",
    "    def common_contexts(self, words, fail_on_unknown=False):\n",
    "        \"\"\"\n",
    "        Find contexts where the specified words can all appear; and\n",
    "        return a frequency distribution mapping each context to the\n",
    "        number of times that context was used.\n",
    "\n",
    "        :param words: The words used to seed the similarity search\n",
    "        :type words: str\n",
    "        :param fail_on_unknown: If true, then raise a value error if\n",
    "            any of the given words do not occur at all in the index.\n",
    "        \"\"\"\n",
    "        words = [self._key(w) for w in words]\n",
    "        contexts = [set(self._word_to_contexts[w]) for w in words]\n",
    "        empty = [words[i] for i in range(len(words)) if not contexts[i]]\n",
    "        common = reduce(set.intersection, contexts)\n",
    "        if empty and fail_on_unknown:\n",
    "            raise ValueError(\"The following word(s) were not found:\", \" \".join(words))\n",
    "        elif not common:\n",
    "            # nothing in common -- just return an empty freqdist.\n",
    "            return FreqDist()\n",
    "        else:\n",
    "            fd = FreqDist(\n",
    "                c for w in words for c in self._word_to_contexts[w] if c in common\n",
    "            )\n",
    "            return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@python_2_unicode_compatible\n",
    "class ConcordanceIndex(object):\n",
    "    \"\"\"\n",
    "    An index that can be used to look up the offset locations at which\n",
    "    a given word occurs in a document.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokens, key=lambda x: x):\n",
    "        \"\"\"\n",
    "        Construct a new concordance index.\n",
    "\n",
    "        :param tokens: The document (list of tokens) that this\n",
    "            concordance index was created from.  This list can be used\n",
    "            to access the context of a given word occurrence.\n",
    "        :param key: A function that maps each token to a normalized\n",
    "            version that will be used as a key in the index.  E.g., if\n",
    "            you use ``key=lambda s:s.lower()``, then the index will be\n",
    "            case-insensitive.\n",
    "        \"\"\"\n",
    "        self._tokens = tokens\n",
    "        \"\"\"The document (list of tokens) that this concordance index\n",
    "           was created from.\"\"\"\n",
    "\n",
    "        self._key = key\n",
    "        \"\"\"Function mapping each token to an index key (or None).\"\"\"\n",
    "\n",
    "        self._offsets = defaultdict(list)\n",
    "        \"\"\"Dictionary mapping words (or keys) to lists of offset indices.\"\"\"\n",
    "        # Initialize the index (self._offsets)\n",
    "        for index, word in enumerate(tokens):\n",
    "            word = self._key(word)\n",
    "            self._offsets[word].append(index)\n",
    "\n",
    "    def tokens(self):\n",
    "        \"\"\"\n",
    "        :rtype: list(str)\n",
    "        :return: The document that this concordance index was\n",
    "            created from.\n",
    "        \"\"\"\n",
    "        return self._tokens\n",
    "\n",
    "\n",
    "    def offsets(self, word):\n",
    "        \"\"\"\n",
    "        :rtype: list(int)\n",
    "        :return: A list of the offset positions at which the given\n",
    "            word occurs.  If a key function was specified for the\n",
    "            index, then given word's key will be looked up.\n",
    "        \"\"\"\n",
    "        word = self._key(word)\n",
    "        return self._offsets[word]\n",
    "\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<ConcordanceIndex for %d tokens (%d types)>' % (\n",
    "            len(self._tokens),\n",
    "            len(self._offsets),\n",
    "        )\n",
    "\n",
    "    def find_concordance(self, word, width=80):\n",
    "        \"\"\"\n",
    "        Find all concordance lines given the query word.\n",
    "        \"\"\"\n",
    "        half_width = (width - len(word) - 2) // 2\n",
    "        context = width // 4  # approx number of words of context\n",
    "\n",
    "        # Find the instances of the word to create the ConcordanceLine\n",
    "        concordance_list = []\n",
    "        offsets = self.offsets(word)\n",
    "        if offsets:\n",
    "            for i in offsets:\n",
    "                query_word = self._tokens[i]\n",
    "                # Find the context of query word.\n",
    "                left_context = self._tokens[max(0, i - context) : i]\n",
    "                right_context = self._tokens[i + 1 : i + context]\n",
    "                # Create the pretty lines with the query_word in the middle.\n",
    "                left_print = ' '.join(left_context)[-half_width:]\n",
    "                right_print = ' '.join(right_context)[:half_width]\n",
    "                # The WYSIWYG line of the concordance.\n",
    "                line_print = ' '.join([left_print, query_word, right_print])\n",
    "                # Create the ConcordanceLine\n",
    "                concordance_line = ConcordanceLine(\n",
    "                    left_context,\n",
    "                    query_word,\n",
    "                    right_context,\n",
    "                    i,\n",
    "                    left_print,\n",
    "                    right_print,\n",
    "                    line_print,\n",
    "                )\n",
    "                concordance_list.append(concordance_line)\n",
    "        return concordance_list\n",
    "\n",
    "\n",
    "    def print_concordance(self, word, width=80, lines=25):\n",
    "        \"\"\"\n",
    "        Print concordance lines given the query word.\n",
    "        :param word: The target word\n",
    "        :type word: str\n",
    "        :param lines: The number of lines to display (default=25)\n",
    "        :type lines: int\n",
    "        :param width: The width of each line, in characters (default=80)\n",
    "        :type width: int\n",
    "        :param save: The option to save the concordance.\n",
    "        :type save: bool\n",
    "        \"\"\"\n",
    "        concordance_list = self.find_concordance(word, width=width)\n",
    "\n",
    "        if not concordance_list:\n",
    "            print(\"no matches\")\n",
    "        else:\n",
    "            lines = min(lines, len(concordance_list))\n",
    "            print(\"Displaying {} of {} matches:\".format(lines, len(concordance_list)))\n",
    "            for i, concordance_line in enumerate(concordance_list[:lines]):\n",
    "                print(concordance_line.line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def demo():\n",
    "    from nltk.corpus import brown\n",
    "\n",
    "    text = Text(brown.words(categories='news'))\n",
    "    print(text)\n",
    "    print()\n",
    "    print(\"Concordance:\")\n",
    "    text.concordance('news')\n",
    "    print()\n",
    "    print(\"Distributionally similar words:\")\n",
    "    text.similar('news')\n",
    "    print()\n",
    "    print(\"Collocations:\")\n",
    "    text.collocations()\n",
    "    print()\n",
    "    # print(\"Automatically generated text:\")\n",
    "    # text.generate()\n",
    "    # print()\n",
    "    print(\"Dispersion plot:\")\n",
    "    text.dispersion_plot(['news', 'report', 'said', 'announced'])\n",
    "    print()\n",
    "    print(\"Vocabulary plot:\")\n",
    "    text.plot(50)\n",
    "    print()\n",
    "    print(\"Indexing:\")\n",
    "    print(\"text[3]:\", text[3])\n",
    "    print(\"text[3:5]:\", text[3:5])\n",
    "    print(\"text.vocab()['news']:\", text.vocab()['news'])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    demo()\n",
    "__all__ = [\n",
    "    \"ContextIndex\",\n",
    "    \"ConcordanceIndex\",\n",
    "    \"TokenSearcher\",\n",
    "    \"Text\",\n",
    "    \"TextCollection\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown\n",
    "text = Text(brown.words(categories='news'))\n",
    "len(text.tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> import nltk\n",
    ">>> sentence = \"\"\"At eight o'clock on Thursday morning\n",
    "... Arthur didn't feel very good.\"\"\"\n",
    ">>> tokens = nltk.word_tokenize(sentence)\n",
    ">>> tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
